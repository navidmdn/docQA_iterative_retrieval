seed_everything: 1

trainer:
  accelerator: gpu
  devices: [1]
  gradient_clip_val: 0.5
  max_epochs: 30
  log_every_n_steps: 25
  val_check_interval: 0.1
  precision: 16
  # run validation before training
  # num_sanity_val_steps: -1

# it's the default behavior
#  logger:
#    class_path: lightning.pytorch.loggers.tensorboard.TensorBoardLogger
#    init_args:
#      save_dir: lightning_logs
  gradient_clip_algorithm: norm
  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        filename: "{epoch}-{rss_avg:.2f}-{train_loss:.2f}"
        save_top_k: 1
        monitor: rss_avg
        mode: max
        auto_insert_metric_name: false

model:
  lr: 2e-5
  warmup_steps: 200
  retriever_model_name: roberta-base
  huggingface_cache_dir: /user/smadani/navid/huggingface_cache

data:
  batch_size: 20
  num_workers: 8
  max_c_len: 300
  max_q_len: 70
  max_q_sp_len: 350
  train_path: data/hotpot/hotpot_train_with_neg_v0.json
  dev_path: data/hotpot/hotpot_dev_with_neg_v0.json
  tokenizer_cp: roberta-base
  preprocessed_data_dir: data/hotpot/preprocessed_train_dataset
  huggingface_cache_dir: /user/smadani/navid/huggingface_cache